{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCnFgzic3kRs"
      },
      "source": [
        "# Programming Exercise Week 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irMSxPmX3kRt"
      },
      "source": [
        "You can download the data [here](https://drive.google.com/file/d/1dpuJKyX6vvSRGDTiRRHsk5OChK23joRA/view?usp=share_link). Please do not hesitate to contact Xiaochen Zheng by [this email](mailto:xzheng@ethz.ch) if you have any question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDYT1AN3kRu"
      },
      "source": [
        "The path to your data folder, where the data is saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mychvOTUu3sI"
      },
      "outputs": [],
      "source": [
        "pwd = '/content/drive/MyDrive/data'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We8zRtCNNwg8",
        "outputId": "d475d1d0-e22f-49aa-efa2-ef3009b8de2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RuMQ6Vr3kRv"
      },
      "source": [
        "We will use a special package to load and investigate the dataset, the [pandas](https://pandas.pydata.org/docs/) library. Take a look at the documentation to see all the options!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MX-mQoHMu0Vc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd    # Package to load and investigate data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg7B2PQX8GID"
      },
      "source": [
        "## Dataset description\n",
        "\n",
        "In this exercise, we want to design a machine learning/deep learning algorithm to help determine/predict whether a patient is non-diabetic (int `0`) or diabetic (int `1`). Each patient is identified with a unique patient ID (pid). In `full_data_train.csv`, medical, demographic, and diagnosis data for each patient is arranged in 20 consecutive rows. Research has identified the following as **important risk factors** for diabetes:\n",
        "\n",
        "```high blood pressure, high cholesterol, smoking, obesity, age and sex, race, diet, exercise, alcohol consumption, BMI, household income, marital status, sleep, time since last checkup, education, health care coverage, mental Health```\n",
        "\n",
        "Given these risk factors, we selected features from a open survey of diabetes related to these risk factors.\n",
        "\n",
        "\n",
        "### Features\n",
        "\n",
        "`Diabetes_binary`\n",
        "\n",
        "(Ever diagonsed) diabetes \n",
        "\n",
        "`HighBP` -> `Bool`\n",
        "\n",
        "High Blood Pressure\n",
        "\n",
        "`HighChol` -> `Bool`\n",
        "\n",
        "High Cholesterol\n",
        "\n",
        "`CholCheck` -> `Bool`\n",
        "\n",
        "Cholesterol check within past five years\n",
        "\n",
        "`BMI` -> `Float`\n",
        "\n",
        "Body Mass Index (BMI)\n",
        "\n",
        "`Smoker` -> `Bool`\n",
        "\n",
        "Have you smoked at least 100 cigarettes (5 packs) in your entire life? \n",
        "\n",
        "`Stroke` -> `Bool`\n",
        "\n",
        "(Ever diagosed) stroke. \n",
        "\n",
        "`HeartDiseaseorAttack` -> `Bool`\n",
        "\n",
        "Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI)\n",
        "\n",
        "`PhysActivity` -> `Bool`\n",
        "\n",
        "Adults who reported doing physical activity or exercise during the past 30 days other than their regular job\n",
        "\n",
        "`Fruits` -> `Bool`\n",
        "\n",
        "Consume Fruit 1 or more times per day \n",
        "\n",
        "`Veggies` -> `Bool`\n",
        "\n",
        "Consume Vegetables 1 or more times per day \n",
        "\n",
        "`HvyAlcoholConsump` -> `Bool`\n",
        "\n",
        "Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)\n",
        "\n",
        "`AnyHealthcare` -> `Bool`\n",
        "\n",
        "Do you have any kind of health care coverage, including health insurance, prepaid plans such as HMOs, or government plans such as Medicare, or Indian Health Service? \n",
        "\n",
        "`NoDocbcCost` -> `Bool`\n",
        "\n",
        "Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?\n",
        "\n",
        "`GenHlth` -> `Int`\n",
        "\n",
        "Would you say that in general your health is between 5 (highest) and 1 (lowest).\n",
        "\n",
        "`MentHlth` -> `Int`\n",
        "\n",
        "Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good? \n",
        "\n",
        "`PhysHlth` -> `Int`\n",
        "\n",
        "Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? \n",
        "\n",
        "`DiffWalk` -> `Int`\n",
        "\n",
        "Do you have serious difficulty walking or climbing stairs? \n",
        "\n",
        "\n",
        "`Sex`, and `Age` -> `Int`\n",
        "\n",
        "`Education` -> `Int`\n",
        "\n",
        "This is already an ordinal variable with 1 being never attended school or kindergarten only up to 6 being college 4 years or more\n",
        "\n",
        "\n",
        "`Income` -> `Int`\n",
        "\n",
        "Variable is already ordinal with 1 being less than \\$10,000 all the way up to 8 being \\$75,000 or more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kksZugxTuYXv"
      },
      "source": [
        "### Load the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e8wTKmJ1uXaY"
      },
      "outputs": [],
      "source": [
        "# Training dataset\n",
        "full_train = pd.read_csv(os.path.join(pwd, 'full_data_train.csv'))\n",
        "# Test dataset\n",
        "X_test = pd.read_csv(os.path.join(pwd, 'indicators_test.csv'))\n",
        "y_test = pd.read_csv(os.path.join(pwd, 'y_test.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTpnXjqw647"
      },
      "source": [
        "### Check the raw data\n",
        "\n",
        "Use ```pandas.DataFrame.info``` to describe null values, data type, memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6cDNf9U1vFHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "becfe9ec-3218-4d36-df5c-587a3ab10d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 228312 entries, 0 to 228311\n",
            "Data columns (total 23 columns):\n",
            " #   Column                Non-Null Count   Dtype  \n",
            "---  ------                --------------   -----  \n",
            " 0   PID                   228312 non-null  int64  \n",
            " 1   Diabetes_binary       228312 non-null  float64\n",
            " 2   HighBP                228312 non-null  float64\n",
            " 3   HighChol              228312 non-null  float64\n",
            " 4   CholCheck             228312 non-null  float64\n",
            " 5   BMI                   228312 non-null  float64\n",
            " 6   Smoker                228312 non-null  float64\n",
            " 7   Stroke                228312 non-null  float64\n",
            " 8   HeartDiseaseorAttack  228312 non-null  float64\n",
            " 9   PhysActivity          228312 non-null  float64\n",
            " 10  Fruits                228312 non-null  float64\n",
            " 11  Veggies               228312 non-null  float64\n",
            " 12  HvyAlcoholConsump     228312 non-null  float64\n",
            " 13  AnyHealthcare         228312 non-null  float64\n",
            " 14  NoDocbcCost           228312 non-null  float64\n",
            " 15  GenHlth               228312 non-null  float64\n",
            " 16  MentHlth              228312 non-null  float64\n",
            " 17  PhysHlth              228312 non-null  float64\n",
            " 18  DiffWalk              228312 non-null  float64\n",
            " 19  Sex                   228312 non-null  float64\n",
            " 20  Age                   228312 non-null  float64\n",
            " 21  Education             228312 non-null  float64\n",
            " 22  Income                228312 non-null  float64\n",
            "dtypes: float64(22), int64(1)\n",
            "memory usage: 40.1 MB\n"
          ]
        }
      ],
      "source": [
        "full_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mkr96I8jw86V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971af012-ef96-402d-bbd4-97b261594b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25368 entries, 0 to 25367\n",
            "Data columns (total 22 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   PID                   25368 non-null  int64  \n",
            " 1   HighBP                25368 non-null  float64\n",
            " 2   HighChol              25368 non-null  float64\n",
            " 3   CholCheck             25368 non-null  float64\n",
            " 4   BMI                   25368 non-null  float64\n",
            " 5   Smoker                25368 non-null  float64\n",
            " 6   Stroke                25368 non-null  float64\n",
            " 7   HeartDiseaseorAttack  25368 non-null  float64\n",
            " 8   PhysActivity          25368 non-null  float64\n",
            " 9   Fruits                25368 non-null  float64\n",
            " 10  Veggies               25368 non-null  float64\n",
            " 11  HvyAlcoholConsump     25368 non-null  float64\n",
            " 12  AnyHealthcare         25368 non-null  float64\n",
            " 13  NoDocbcCost           25368 non-null  float64\n",
            " 14  GenHlth               25368 non-null  float64\n",
            " 15  MentHlth              25368 non-null  float64\n",
            " 16  PhysHlth              25368 non-null  float64\n",
            " 17  DiffWalk              25368 non-null  float64\n",
            " 18  Sex                   25368 non-null  float64\n",
            " 19  Age                   25368 non-null  float64\n",
            " 20  Education             25368 non-null  float64\n",
            " 21  Income                25368 non-null  float64\n",
            "dtypes: float64(21), int64(1)\n",
            "memory usage: 4.3 MB\n"
          ]
        }
      ],
      "source": [
        "X_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.info()"
      ],
      "metadata": {
        "id": "gpsgZidmNTs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d63b53-1412-4078-d95d-b712d4887c43"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25368 entries, 0 to 25367\n",
            "Data columns (total 2 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   PID              25368 non-null  int64  \n",
            " 1   Diabetes_binary  25368 non-null  float64\n",
            "dtypes: float64(1), int64(1)\n",
            "memory usage: 396.5 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sufPBth2xCG-"
      },
      "source": [
        "## Data Preprocessing\n",
        "Take a look at the raw data and think carefully about what kinds of data preprocessing methods needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hoO8N_LWxJe-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23859ce4-c7ce-4d72-8d71-29a3e38298c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** YOUR CODE HERE. ***'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# You do not necessarily need to do anything here, this is just to provide some space to look at the dataset's properties and contents.\n",
        "\"*** YOUR CODE HERE. ***\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqw6N2xU3kRy"
      },
      "source": [
        "### Task 1\n",
        "Notice that there is one column name **PID** in both *full_train* and *X_test*. Why should we better remove this from the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qwfuz_t3kRy"
      },
      "source": [
        "Your Answer: Because \"PID\" stands for \"patient ID\", which is irrelevant to determining/predicting whether a patient is diabetic or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ey4LSb33kRy"
      },
      "source": [
        "### Task 2\n",
        " Use the pandas `drop` function to remove the PID column in test and training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZvMi-6Lh3kRy"
      },
      "outputs": [],
      "source": [
        "full_train = full_train.drop(['PID'], axis=1)\n",
        "X_test = X_test.drop(['PID'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxbW8_zT3kRy"
      },
      "source": [
        "### Task 3\n",
        "Separate the labels in the column `Diabetes_binary` from the training set and create a new tensor `y_train` and `y_test` containing the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6t_IKIB63kRz"
      },
      "outputs": [],
      "source": [
        "X_train = full_train.drop(['Diabetes_binary'], axis=1).to_numpy()\n",
        "y_train = full_train['Diabetes_binary'].to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "y_test = y_test['Diabetes_binary'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzHdDVFn6G-e"
      },
      "source": [
        "### Task 4 - Standardization Scaling\n",
        "Notice that different features have different scales. For example, `BMI` ranges from 12.0 to 98.0 and `Age` ranges from 1.0 to 13.0. Normalization is a data preparation technique that is frequently used in machine learning to deal with data with different scales.\n",
        "\n",
        "Here you will apply **standardization scaling**. The term **standardization** refers to the process of centering a variable at zero and standardizing the variance at one. Subtracting the mean of each observation and then dividing by the standard deviation is the procedure. The features will be rescaled so that they have the attributes of a typical normal distribution with standard deviations.\n",
        "\n",
        "***Hint***: Use `numpy` or `Standardscaler` provided by `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y4QiZ88gxaj1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "stdScaler = StandardScaler()\n",
        "X_train = stdScaler.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui8yvjJn3kRz"
      },
      "source": [
        "### Task 5 - Data Structure\n",
        "By applying `pd.read_csv()`, you store your data in `pandas.DataFrame`. After finishing task 4, you should store your data in `numpy.ndarray`. But for `torch.nn.Module`, you need to transfer your data to the data type `torch.Tensor`.\n",
        "\n",
        "***Hint***: Try to learn and apply [torch.from_numpy()](https://pytorch.org/docs/stable/generated/torch.from_numpy.html), [torch.Tensor.to()](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html), [torch.tensor()](https://pytorch.org/docs/stable/generated/torch.tensor.html).\n",
        "\n",
        "Transform the variables `X_train`, `X_test`, `y_train` and `y_test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S2DPt5G23kRz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "X_train = torch.from_numpy(X_train).to(dtype=torch.float)\n",
        "y_train = torch.from_numpy(y_train).to(dtype=torch.long)\n",
        "\n",
        "X_test = torch.from_numpy(X_test).to(dtype=torch.float)\n",
        "y_test = torch.from_numpy(y_test).to(dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWnI2naSnBAE"
      },
      "source": [
        "## Deep learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GpGc1_U3kRz"
      },
      "source": [
        "### Task 6\n",
        "Finish the deep learning skeleton step-by-step.\n",
        "\n",
        "***Hint***: \n",
        "\n",
        "(1) Implement a multilayer perceptron with several [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (e.g. 4 linear layers) followed by [relu activation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU). We show you an example of the first layer.\n",
        "\n",
        "(2) Take regularization into account and implement certain layers to avoid overfitting. e.g. dropout\n",
        "\n",
        "(3) Make sure that the output's size of one layer should match the input's size of the following/subsequent layer by checking `tensor.shape`.\n",
        "\n",
        "(4) Make sure that your model's output should have the size of ($N$, 2), where $N$ is the batch size. 2 represents the possible outcome state of the model e.g. diabetic or non-diabetic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GLD1xE4lvY55"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class YourModel(torch.nn.Module):\n",
        "    \"\"\" Your model should inherite from torch.nn.Module.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Fully-connected (dense) layers\n",
        "        self.fc1 = nn.Linear(21 , 64 )\n",
        "        self.fc2 = nn.Linear(64 , 512)\n",
        "        self.fc3 = nn.Linear(512, 512)\n",
        "        self.fc4 = nn.Linear(512, 2  )\n",
        "        # Drop-out layers\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Forward pass.'''\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "huZG0JaA3kR0"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Perform forward pass\n",
        "        outputs = model(data)\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, target)\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "        # Perform optimization\n",
        "        optimizer.step()\n",
        "        # Printing\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    \n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "J6u1Y64wWxHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08d49f9-2e5e-4343-f54c-b17d38dac1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Train Epoch: 1 [0/228312 (0%)]\tLoss: 0.698828\n",
            "Train Epoch: 1 [25600/228312 (11%)]\tLoss: 0.356995\n",
            "Train Epoch: 1 [51200/228312 (22%)]\tLoss: 0.289147\n",
            "Train Epoch: 1 [76800/228312 (34%)]\tLoss: 0.333199\n",
            "Train Epoch: 1 [102400/228312 (45%)]\tLoss: 0.346933\n",
            "Train Epoch: 1 [128000/228312 (56%)]\tLoss: 0.300793\n",
            "Train Epoch: 1 [153600/228312 (67%)]\tLoss: 0.328935\n",
            "Train Epoch: 1 [179200/228312 (78%)]\tLoss: 0.336842\n",
            "Train Epoch: 1 [204800/228312 (90%)]\tLoss: 0.234472\n",
            "\n",
            "Test set: Average loss: 0.6751, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 2\n",
            "Train Epoch: 2 [0/228312 (0%)]\tLoss: 0.299164\n",
            "Train Epoch: 2 [25600/228312 (11%)]\tLoss: 0.262333\n",
            "Train Epoch: 2 [51200/228312 (22%)]\tLoss: 0.338873\n",
            "Train Epoch: 2 [76800/228312 (34%)]\tLoss: 0.280064\n",
            "Train Epoch: 2 [102400/228312 (45%)]\tLoss: 0.335847\n",
            "Train Epoch: 2 [128000/228312 (56%)]\tLoss: 0.281099\n",
            "Train Epoch: 2 [153600/228312 (67%)]\tLoss: 0.314652\n",
            "Train Epoch: 2 [179200/228312 (78%)]\tLoss: 0.378633\n",
            "Train Epoch: 2 [204800/228312 (90%)]\tLoss: 0.321737\n",
            "\n",
            "Test set: Average loss: 0.6381, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 3\n",
            "Train Epoch: 3 [0/228312 (0%)]\tLoss: 0.323255\n",
            "Train Epoch: 3 [25600/228312 (11%)]\tLoss: 0.247864\n",
            "Train Epoch: 3 [51200/228312 (22%)]\tLoss: 0.239894\n",
            "Train Epoch: 3 [76800/228312 (34%)]\tLoss: 0.326055\n",
            "Train Epoch: 3 [102400/228312 (45%)]\tLoss: 0.288789\n",
            "Train Epoch: 3 [128000/228312 (56%)]\tLoss: 0.324559\n",
            "Train Epoch: 3 [153600/228312 (67%)]\tLoss: 0.355841\n",
            "Train Epoch: 3 [179200/228312 (78%)]\tLoss: 0.289451\n",
            "Train Epoch: 3 [204800/228312 (90%)]\tLoss: 0.342694\n",
            "\n",
            "Test set: Average loss: 0.5610, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 4\n",
            "Train Epoch: 4 [0/228312 (0%)]\tLoss: 0.323870\n",
            "Train Epoch: 4 [25600/228312 (11%)]\tLoss: 0.284406\n",
            "Train Epoch: 4 [51200/228312 (22%)]\tLoss: 0.335442\n",
            "Train Epoch: 4 [76800/228312 (34%)]\tLoss: 0.217692\n",
            "Train Epoch: 4 [102400/228312 (45%)]\tLoss: 0.324811\n",
            "Train Epoch: 4 [128000/228312 (56%)]\tLoss: 0.341192\n",
            "Train Epoch: 4 [153600/228312 (67%)]\tLoss: 0.311476\n",
            "Train Epoch: 4 [179200/228312 (78%)]\tLoss: 0.335366\n",
            "Train Epoch: 4 [204800/228312 (90%)]\tLoss: 0.299878\n",
            "\n",
            "Test set: Average loss: 0.5712, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 5\n",
            "Train Epoch: 5 [0/228312 (0%)]\tLoss: 0.243726\n",
            "Train Epoch: 5 [25600/228312 (11%)]\tLoss: 0.281117\n",
            "Train Epoch: 5 [51200/228312 (22%)]\tLoss: 0.307327\n",
            "Train Epoch: 5 [76800/228312 (34%)]\tLoss: 0.309370\n",
            "Train Epoch: 5 [102400/228312 (45%)]\tLoss: 0.307314\n",
            "Train Epoch: 5 [128000/228312 (56%)]\tLoss: 0.390351\n",
            "Train Epoch: 5 [153600/228312 (67%)]\tLoss: 0.376346\n",
            "Train Epoch: 5 [179200/228312 (78%)]\tLoss: 0.319144\n",
            "Train Epoch: 5 [204800/228312 (90%)]\tLoss: 0.301466\n",
            "\n",
            "Test set: Average loss: 0.4738, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 6\n",
            "Train Epoch: 6 [0/228312 (0%)]\tLoss: 0.372805\n",
            "Train Epoch: 6 [25600/228312 (11%)]\tLoss: 0.415090\n",
            "Train Epoch: 6 [51200/228312 (22%)]\tLoss: 0.277159\n",
            "Train Epoch: 6 [76800/228312 (34%)]\tLoss: 0.284099\n",
            "Train Epoch: 6 [102400/228312 (45%)]\tLoss: 0.363858\n",
            "Train Epoch: 6 [128000/228312 (56%)]\tLoss: 0.325516\n",
            "Train Epoch: 6 [153600/228312 (67%)]\tLoss: 0.349740\n",
            "Train Epoch: 6 [179200/228312 (78%)]\tLoss: 0.276478\n",
            "Train Epoch: 6 [204800/228312 (90%)]\tLoss: 0.323892\n",
            "\n",
            "Test set: Average loss: 0.5019, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 7\n",
            "Train Epoch: 7 [0/228312 (0%)]\tLoss: 0.317231\n",
            "Train Epoch: 7 [25600/228312 (11%)]\tLoss: 0.308309\n",
            "Train Epoch: 7 [51200/228312 (22%)]\tLoss: 0.311735\n",
            "Train Epoch: 7 [76800/228312 (34%)]\tLoss: 0.348885\n",
            "Train Epoch: 7 [102400/228312 (45%)]\tLoss: 0.290333\n",
            "Train Epoch: 7 [128000/228312 (56%)]\tLoss: 0.329527\n",
            "Train Epoch: 7 [153600/228312 (67%)]\tLoss: 0.333195\n",
            "Train Epoch: 7 [179200/228312 (78%)]\tLoss: 0.294724\n",
            "Train Epoch: 7 [204800/228312 (90%)]\tLoss: 0.296498\n",
            "\n",
            "Test set: Average loss: 0.5126, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 8\n",
            "Train Epoch: 8 [0/228312 (0%)]\tLoss: 0.336941\n",
            "Train Epoch: 8 [25600/228312 (11%)]\tLoss: 0.330132\n",
            "Train Epoch: 8 [51200/228312 (22%)]\tLoss: 0.292406\n",
            "Train Epoch: 8 [76800/228312 (34%)]\tLoss: 0.354362\n",
            "Train Epoch: 8 [102400/228312 (45%)]\tLoss: 0.274018\n",
            "Train Epoch: 8 [128000/228312 (56%)]\tLoss: 0.326225\n",
            "Train Epoch: 8 [153600/228312 (67%)]\tLoss: 0.248149\n",
            "Train Epoch: 8 [179200/228312 (78%)]\tLoss: 0.271409\n",
            "Train Epoch: 8 [204800/228312 (90%)]\tLoss: 0.299600\n",
            "\n",
            "Test set: Average loss: 0.5340, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 9\n",
            "Train Epoch: 9 [0/228312 (0%)]\tLoss: 0.295009\n",
            "Train Epoch: 9 [25600/228312 (11%)]\tLoss: 0.283828\n",
            "Train Epoch: 9 [51200/228312 (22%)]\tLoss: 0.255630\n",
            "Train Epoch: 9 [76800/228312 (34%)]\tLoss: 0.291777\n",
            "Train Epoch: 9 [102400/228312 (45%)]\tLoss: 0.311599\n",
            "Train Epoch: 9 [128000/228312 (56%)]\tLoss: 0.350647\n",
            "Train Epoch: 9 [153600/228312 (67%)]\tLoss: 0.367048\n",
            "Train Epoch: 9 [179200/228312 (78%)]\tLoss: 0.369927\n",
            "Train Epoch: 9 [204800/228312 (90%)]\tLoss: 0.343237\n",
            "\n",
            "Test set: Average loss: 0.5157, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 10\n",
            "Train Epoch: 10 [0/228312 (0%)]\tLoss: 0.331876\n",
            "Train Epoch: 10 [25600/228312 (11%)]\tLoss: 0.292656\n",
            "Train Epoch: 10 [51200/228312 (22%)]\tLoss: 0.312707\n",
            "Train Epoch: 10 [76800/228312 (34%)]\tLoss: 0.249366\n",
            "Train Epoch: 10 [102400/228312 (45%)]\tLoss: 0.340893\n",
            "Train Epoch: 10 [128000/228312 (56%)]\tLoss: 0.261034\n",
            "Train Epoch: 10 [153600/228312 (67%)]\tLoss: 0.353523\n",
            "Train Epoch: 10 [179200/228312 (78%)]\tLoss: 0.311876\n",
            "Train Epoch: 10 [204800/228312 (90%)]\tLoss: 0.328261\n",
            "\n",
            "Test set: Average loss: 0.4588, Accuracy: 21752/25368 (86%)\n",
            "\n",
            "Starting epoch 11\n",
            "Train Epoch: 11 [0/228312 (0%)]\tLoss: 0.339848\n",
            "Train Epoch: 11 [25600/228312 (11%)]\tLoss: 0.304059\n",
            "Train Epoch: 11 [51200/228312 (22%)]\tLoss: 0.318451\n",
            "Train Epoch: 11 [76800/228312 (34%)]\tLoss: 0.298296\n",
            "Train Epoch: 11 [102400/228312 (45%)]\tLoss: 0.276971\n",
            "Train Epoch: 11 [128000/228312 (56%)]\tLoss: 0.364815\n",
            "Train Epoch: 11 [153600/228312 (67%)]\tLoss: 0.275757\n",
            "Train Epoch: 11 [179200/228312 (78%)]\tLoss: 0.270991\n",
            "Train Epoch: 11 [204800/228312 (90%)]\tLoss: 0.348953\n",
            "\n",
            "Test set: Average loss: 0.5183, Accuracy: 20489/25368 (81%)\n",
            "\n",
            "Starting epoch 12\n",
            "Train Epoch: 12 [0/228312 (0%)]\tLoss: 0.332004\n",
            "Train Epoch: 12 [25600/228312 (11%)]\tLoss: 0.357995\n",
            "Train Epoch: 12 [51200/228312 (22%)]\tLoss: 0.275933\n",
            "Train Epoch: 12 [76800/228312 (34%)]\tLoss: 0.323978\n",
            "Train Epoch: 12 [102400/228312 (45%)]\tLoss: 0.360924\n",
            "Train Epoch: 12 [128000/228312 (56%)]\tLoss: 0.301671\n",
            "Train Epoch: 12 [153600/228312 (67%)]\tLoss: 0.408901\n",
            "Train Epoch: 12 [179200/228312 (78%)]\tLoss: 0.354808\n",
            "Train Epoch: 12 [204800/228312 (90%)]\tLoss: 0.294630\n",
            "\n",
            "Test set: Average loss: 0.5171, Accuracy: 20392/25368 (80%)\n",
            "\n",
            "Starting epoch 13\n",
            "Train Epoch: 13 [0/228312 (0%)]\tLoss: 0.326570\n",
            "Train Epoch: 13 [25600/228312 (11%)]\tLoss: 0.298332\n",
            "Train Epoch: 13 [51200/228312 (22%)]\tLoss: 0.343597\n",
            "Train Epoch: 13 [76800/228312 (34%)]\tLoss: 0.329869\n",
            "Train Epoch: 13 [102400/228312 (45%)]\tLoss: 0.345181\n",
            "Train Epoch: 13 [128000/228312 (56%)]\tLoss: 0.287004\n",
            "Train Epoch: 13 [153600/228312 (67%)]\tLoss: 0.355738\n",
            "Train Epoch: 13 [179200/228312 (78%)]\tLoss: 0.298144\n",
            "Train Epoch: 13 [204800/228312 (90%)]\tLoss: 0.287252\n",
            "\n",
            "Test set: Average loss: 0.5486, Accuracy: 20594/25368 (81%)\n",
            "\n",
            "Starting epoch 14\n",
            "Train Epoch: 14 [0/228312 (0%)]\tLoss: 0.359496\n",
            "Train Epoch: 14 [25600/228312 (11%)]\tLoss: 0.308943\n",
            "Train Epoch: 14 [51200/228312 (22%)]\tLoss: 0.321656\n",
            "Train Epoch: 14 [76800/228312 (34%)]\tLoss: 0.289550\n",
            "Train Epoch: 14 [102400/228312 (45%)]\tLoss: 0.334464\n",
            "Train Epoch: 14 [128000/228312 (56%)]\tLoss: 0.344292\n",
            "Train Epoch: 14 [153600/228312 (67%)]\tLoss: 0.330495\n",
            "Train Epoch: 14 [179200/228312 (78%)]\tLoss: 0.361830\n",
            "Train Epoch: 14 [204800/228312 (90%)]\tLoss: 0.273754\n",
            "\n",
            "Test set: Average loss: 0.5286, Accuracy: 20613/25368 (81%)\n",
            "\n",
            "Starting epoch 15\n",
            "Train Epoch: 15 [0/228312 (0%)]\tLoss: 0.293425\n",
            "Train Epoch: 15 [25600/228312 (11%)]\tLoss: 0.339319\n",
            "Train Epoch: 15 [51200/228312 (22%)]\tLoss: 0.398233\n",
            "Train Epoch: 15 [76800/228312 (34%)]\tLoss: 0.322344\n",
            "Train Epoch: 15 [102400/228312 (45%)]\tLoss: 0.332508\n",
            "Train Epoch: 15 [128000/228312 (56%)]\tLoss: 0.306486\n",
            "Train Epoch: 15 [153600/228312 (67%)]\tLoss: 0.339606\n",
            "Train Epoch: 15 [179200/228312 (78%)]\tLoss: 0.370809\n",
            "Train Epoch: 15 [204800/228312 (90%)]\tLoss: 0.290274\n",
            "\n",
            "Test set: Average loss: 0.5306, Accuracy: 20143/25368 (79%)\n",
            "\n",
            "Starting epoch 16\n",
            "Train Epoch: 16 [0/228312 (0%)]\tLoss: 0.280829\n",
            "Train Epoch: 16 [25600/228312 (11%)]\tLoss: 0.361148\n",
            "Train Epoch: 16 [51200/228312 (22%)]\tLoss: 0.300440\n",
            "Train Epoch: 16 [76800/228312 (34%)]\tLoss: 0.360286\n",
            "Train Epoch: 16 [102400/228312 (45%)]\tLoss: 0.296562\n",
            "Train Epoch: 16 [128000/228312 (56%)]\tLoss: 0.310506\n",
            "Train Epoch: 16 [153600/228312 (67%)]\tLoss: 0.329145\n",
            "Train Epoch: 16 [179200/228312 (78%)]\tLoss: 0.320416\n",
            "Train Epoch: 16 [204800/228312 (90%)]\tLoss: 0.300842\n",
            "\n",
            "Test set: Average loss: 0.5750, Accuracy: 20377/25368 (80%)\n",
            "\n",
            "Starting epoch 17\n",
            "Train Epoch: 17 [0/228312 (0%)]\tLoss: 0.292420\n",
            "Train Epoch: 17 [25600/228312 (11%)]\tLoss: 0.311818\n",
            "Train Epoch: 17 [51200/228312 (22%)]\tLoss: 0.258716\n",
            "Train Epoch: 17 [76800/228312 (34%)]\tLoss: 0.321569\n",
            "Train Epoch: 17 [102400/228312 (45%)]\tLoss: 0.264178\n",
            "Train Epoch: 17 [128000/228312 (56%)]\tLoss: 0.305344\n",
            "Train Epoch: 17 [153600/228312 (67%)]\tLoss: 0.319083\n",
            "Train Epoch: 17 [179200/228312 (78%)]\tLoss: 0.372230\n",
            "Train Epoch: 17 [204800/228312 (90%)]\tLoss: 0.282908\n",
            "\n",
            "Test set: Average loss: 0.5535, Accuracy: 20895/25368 (82%)\n",
            "\n",
            "Starting epoch 18\n",
            "Train Epoch: 18 [0/228312 (0%)]\tLoss: 0.276025\n",
            "Train Epoch: 18 [25600/228312 (11%)]\tLoss: 0.374004\n",
            "Train Epoch: 18 [51200/228312 (22%)]\tLoss: 0.345264\n",
            "Train Epoch: 18 [76800/228312 (34%)]\tLoss: 0.281600\n",
            "Train Epoch: 18 [102400/228312 (45%)]\tLoss: 0.309341\n",
            "Train Epoch: 18 [128000/228312 (56%)]\tLoss: 0.318397\n",
            "Train Epoch: 18 [153600/228312 (67%)]\tLoss: 0.318860\n",
            "Train Epoch: 18 [179200/228312 (78%)]\tLoss: 0.284865\n",
            "Train Epoch: 18 [204800/228312 (90%)]\tLoss: 0.290483\n",
            "\n",
            "Test set: Average loss: 0.5048, Accuracy: 20150/25368 (79%)\n",
            "\n",
            "Starting epoch 19\n",
            "Train Epoch: 19 [0/228312 (0%)]\tLoss: 0.298864\n",
            "Train Epoch: 19 [25600/228312 (11%)]\tLoss: 0.263716\n",
            "Train Epoch: 19 [51200/228312 (22%)]\tLoss: 0.309952\n",
            "Train Epoch: 19 [76800/228312 (34%)]\tLoss: 0.271213\n",
            "Train Epoch: 19 [102400/228312 (45%)]\tLoss: 0.309245\n",
            "Train Epoch: 19 [128000/228312 (56%)]\tLoss: 0.280980\n",
            "Train Epoch: 19 [153600/228312 (67%)]\tLoss: 0.286657\n",
            "Train Epoch: 19 [179200/228312 (78%)]\tLoss: 0.379565\n",
            "Train Epoch: 19 [204800/228312 (90%)]\tLoss: 0.282572\n",
            "\n",
            "Test set: Average loss: 0.5470, Accuracy: 21460/25368 (85%)\n",
            "\n",
            "Starting epoch 20\n",
            "Train Epoch: 20 [0/228312 (0%)]\tLoss: 0.333475\n",
            "Train Epoch: 20 [25600/228312 (11%)]\tLoss: 0.303627\n",
            "Train Epoch: 20 [51200/228312 (22%)]\tLoss: 0.313946\n",
            "Train Epoch: 20 [76800/228312 (34%)]\tLoss: 0.234028\n",
            "Train Epoch: 20 [102400/228312 (45%)]\tLoss: 0.363444\n",
            "Train Epoch: 20 [128000/228312 (56%)]\tLoss: 0.241532\n",
            "Train Epoch: 20 [153600/228312 (67%)]\tLoss: 0.309129\n",
            "Train Epoch: 20 [179200/228312 (78%)]\tLoss: 0.279235\n",
            "Train Epoch: 20 [204800/228312 (90%)]\tLoss: 0.370568\n",
            "\n",
            "Test set: Average loss: 0.5158, Accuracy: 20187/25368 (80%)\n",
            "\n",
            "Training process has finished.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "def main(train_data, train_label, test_data, test_label, batch_size, epochs):\n",
        "    \"\"\" Training your model.\n",
        "\n",
        "    Args:\n",
        "        train_data, test_data (tensor): The training/testing data. It should have a shape of (n_instance, n_features).\n",
        "        train_label, test_label (tensor): The labels of training/testing instances. It should have a shape of (n_instance, 1).\n",
        "        batch_size  (Union[int, NoneType]): The number of samples loaded for one iteration.\n",
        "        epochs (Union[int, NoneType]): The number of epochs. When this reaches, the training stops.\n",
        "    \"\"\"\n",
        "    # Set fixed random number seed. DO NOT CHANGE IT.\n",
        "    torch.manual_seed(336699)\n",
        "    \n",
        "    # Prepare series dataset.\n",
        "    train_dataset = TensorDataset(train_data, train_label)\n",
        "    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataset  = TensorDataset(test_data, test_label)\n",
        "    test_loader   = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize proposed model.\n",
        "    model = YourModel() \n",
        "\n",
        "    # Define the loss function and optimizer. You can freely choose your loss function and optimizer based on your task.\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    criterion_test = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "    # Run the training loop\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch}')\n",
        "        train(model, train_loader, criterion, optimizer, epoch)\n",
        "        test_loss, test_accuracy = test(model, test_loader, criterion_test)\n",
        "        losses.append(test_loss)\n",
        "        accuracies.append(test_accuracy)\n",
        "        \n",
        "    # Process is complete.\n",
        "    print('Training process has finished.')\n",
        "    \n",
        "    return losses, accuracies\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run your codes here.\n",
        "    losses, accuracies = main(X_train, y_train, X_test, y_test, 256, 20) #exchange with your variabel names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlc4snCn3kR0"
      },
      "source": [
        "Please interpret the resuls of your analysis. What can be learned? What can we utilise the algorithm? Is it visible to use the algorithm in practice, considering the training and test error?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that, as the training process goes, the accuracy of the model converges to a satisfying level (~85%). This suggests that we can use such algorithm to predict/determine whether a patient is diabetic or not. It is visible to use the algorithm in practice, considering the training and test error. However, we should point out that such algorithm should be treated as medical suggestions instead of definitive medical decisions, as the cost would be high if a mis-diagnosis took place."
      ],
      "metadata": {
        "id": "8MSszovBOHhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "3spo_1tNOJuv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = range(1, 21)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss', color=color)\n",
        "ax1.plot(t, losses, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('accuracy (%)', color=color)  # we already handled the x-label with ax1\n",
        "ax2.plot(t, accuracies, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "ax2.set_ylim([0, 100])\n",
        "\n",
        "plt.xticks(list(t))\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AzWEx1TeOKLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "02ef976d-6a32-426f-8514-c2d43c0c4488"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_wVY3CgC5LR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5940b77976ef3bac7cfc78a082ed4676e831584770ac5adf4c28bfa1611ecc43"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}